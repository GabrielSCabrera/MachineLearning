\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{NNWfacedetection}
\citation{NNWsunshine}
\citation{CCdata}
\citation{4}
\citation{5}
\newlabel{FirstPage}{{}{1}{}{section*.1}{}}
\@writefile{toc}{\contentsline {title}{Neural Networks}{1}{section*.2}}
\@writefile{toc}{\contentsline {abstract}{Abstract}{1}{section*.1}}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section*.3}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Theory and Algorithms}{1}{section*.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Linear Regression}{1}{section*.5}}
\citation{5}
\citation{nielsenneural}
\citation{breastcancer}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Ordinary Least Squares}{2}{section*.6}}
\newlabel{eq:MSE}{{3}{2}{}{equation.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Ridge Regression}{2}{section*.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Logistic Regression and Gradient Descent}{2}{section*.8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Stochastic Gradient Descent method}{2}{section*.9}}
\citation{stackexchangeminimum}
\citation{lecturenotes}
\newlabel{eq:sigmoid}{{9}{3}{}{equation.2.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Local minimum vs. Global minimum of non-convex cost function conceptualization figure.}}{3}{figure.1}}
\newlabel{fig:local_min}{{1}{3}{Local minimum vs. Global minimum of non-convex cost function conceptualization figure}{figure.1}{}}
\citation{lecturenotes}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Neural Network Dynamics}{4}{section*.10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Feed-Forward}{4}{section*.11}}
\newlabel{eq:z1}{{18}{4}{}{equation.2.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces An illustration of how the inputs $X$ and weights $W$ and bias $b$ feed into a single neuron. Image taken from Antti Honkela.}}{4}{figure.2}}
\newlabel{fig:MLP_conc}{{2}{4}{An illustration of how the inputs $X$ and weights $W$ and bias $b$ feed into a single neuron. Image taken from Antti Honkela}{figure.2}{}}
\newlabel{eq:a1}{{19}{4}{}{equation.2.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A fully connected ANN illustration of two hidden layers and one output neuron. Image taken from: Vojtech Pavlovsky}}{4}{figure.3}}
\newlabel{fig:MLP_full}{{3}{4}{A fully connected ANN illustration of two hidden layers and one output neuron. Image taken from: Vojtech Pavlovsky}{figure.3}{}}
\newlabel{eq:zWb}{{23}{5}{}{equation.2.23}{}}
\newlabel{eq:afz}{{24}{5}{}{equation.2.24}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Backwards-Propagation}{5}{section*.12}}
\newlabel{eq:NNWdC/dW}{{28}{5}{}{equation.2.28}{}}
\newlabel{eq:NNWdC/db}{{29}{5}{}{equation.2.29}{}}
\newlabel{eq:bp_pdvs}{{30}{5}{}{equation.2.30}{}}
\newlabel{eq:C/zl}{{34}{5}{}{equation.2.34}{}}
\newlabel{eq:C/zl1}{{36}{5}{}{equation.2.36}{}}
\newlabel{eq:C/zl2}{{37}{5}{}{equation.2.37}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3}Learning rate, batches and $l2$ regularization}{6}{section*.13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D}Cost- and Activation functions}{7}{section*.14}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Several cost function examples. Mainly, the cross-entropy and mean-squared error (MSE) functions will be explored in this study. As explained previously, the activations of the final layer $a^L$ are equivalent with the prediction of $y$, namely $\mathaccentV {hat}05E{y}$. }}{7}{table.1}}
\newlabel{tab:cost_functions}{{I}{7}{Several cost function examples. Mainly, the cross-entropy and mean-squared error (MSE) functions will be explored in this study. As explained previously, the activations of the final layer $a^L$ are equivalent with the prediction of $y$, namely $\hat {y}$}{table.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Mean-Squared Error}{7}{section*.15}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Cross-Entropy}{7}{section*.16}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Table listing a number of common activation functions and their derivatives. Several of these are of interest and are implemented into the study.}}{7}{table.2}}
\newlabel{tab:activation_functions}{{II}{7}{Table listing a number of common activation functions and their derivatives. Several of these are of interest and are implemented into the study}{table.2}{}}
\citation{ReLUImagenet}
\citation{He2015}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3}Sigmoid function}{8}{section*.17}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4}Tanh function}{8}{section*.18}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5}ReLU6}{8}{section*.19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6}Weight- and Bias Initialization}{8}{section*.20}}
\newlabel{eq:xavier}{{55}{8}{}{equation.2.55}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {E}Classification}{8}{section*.21}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Neural Network Application}{8}{section*.22}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Accuracy Assertion of Classification}{8}{section*.23}}
\citation{2}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Conceptualized figure of a cumulative gain chart. The figure includes all cases of 0 and 1 predictions, and a perfect 1 model. The model also includes a baseline illustrating what a random $50/50$ pick of 0's and 1's looks like.}}{9}{figure.4}}
\newlabel{fig:conceptual_cumulative_gains}{{4}{9}{Conceptualized figure of a cumulative gain chart. The figure includes all cases of 0 and 1 predictions, and a perfect 1 model. The model also includes a baseline illustrating what a random $50/50$ pick of 0's and 1's looks like}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {F}Regression}{9}{section*.24}}
\newlabel{eq:frankes_function}{{63}{9}{}{equation.2.63}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The \textit  {Franke function} surface plot illustration for $x$ and $y$ values ranging from zero to one.}}{9}{figure.5}}
\newlabel{fig:frankesfunction}{{5}{9}{The \textit {Franke function} surface plot illustration for $x$ and $y$ values ranging from zero to one}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The \textit  {Franke function} surface plot illustration for $x$ and $y$ values ranging from zero to one. Random Gaussian noise with mean value $\mu =0$ and standard deviation $\sigma =0.01$ is added to each data point $f(x_i,y_i)$.}}{10}{figure.6}}
\newlabel{fig:frankesfunction}{{6}{10}{The \textit {Franke function} surface plot illustration for $x$ and $y$ values ranging from zero to one. Random Gaussian noise with mean value $\mu =0$ and standard deviation $\sigma =0.01$ is added to each data point $f(x_i,y_i)$}{figure.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Neural Network Application}{10}{section*.25}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Accuracy Assertion of Regression}{10}{section*.26}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Method}{10}{section*.27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Preprocessing of the Data Set}{10}{section*.28}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Outlier Filtration}{10}{section*.29}}
\citation{CCdata}
\citation{1}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Column Scaling}{11}{section*.30}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3}One-Hot Encoding classifiers}{11}{section*.31}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4}History of past payment columns}{11}{section*.32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Neural Network Design}{11}{section*.33}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Weight- and Bias Initialization}{12}{section*.34}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Hyperparameter and Learning Rate analysis}{12}{section*.35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Classification}{12}{section*.36}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Logistic Regression}{12}{section*.37}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Neural Network: Classification}{12}{section*.38}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D}Regression}{12}{section*.39}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Linear Regression}{13}{section*.40}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Neural Network: Regression}{13}{section*.41}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Results}{13}{section*.42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Classification}{13}{section*.43}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The accurary score of the classification neural network for different $\eta $ and $\lambda $-values after studing the credit card data.}}{13}{figure.7}}
\newlabel{fig:cc_acc}{{7}{13}{The accurary score of the classification neural network for different $\eta $ and $\lambda $-values after studing the credit card data}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The F1-score of the classification neural network after studing the credit card data.}}{13}{figure.8}}
\newlabel{fig:cc_F1}{{8}{13}{The F1-score of the classification neural network after studing the credit card data}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The AUC-score of the classification neural network after studing the credit card data.}}{13}{figure.9}}
\newlabel{fig:cc_auc}{{9}{13}{The AUC-score of the classification neural network after studing the credit card data}{figure.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The Gains-Ratio of the classification neural network after studing the credit card data.}}{14}{figure.10}}
\newlabel{fig:cc_gr}{{10}{14}{The Gains-Ratio of the classification neural network after studing the credit card data}{figure.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Regression}{14}{section*.44}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces The mean squared error of the regression neural network after studing Franke's function.}}{14}{figure.11}}
\newlabel{fig:ff_mse}{{11}{14}{The mean squared error of the regression neural network after studing Franke's function}{figure.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces The $R^2$-score of the regression neural network after studing Franke's function.}}{14}{figure.12}}
\newlabel{fig:ff_r2}{{12}{14}{The $R^2$-score of the regression neural network after studing Franke's function}{figure.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Discussion}{14}{section*.45}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Classification}{14}{section*.46}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Regression}{14}{section*.47}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Comparison of the Methods}{14}{section*.48}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusion}{14}{section*.49}}
\bibdata{mainNotes,ref}
\bibcite{2}{{1}{n.d.}{{\em  \text  {Classification Accuracy is Not Enough: More Performance Measures You Can Use By Jason Brownlee}}}{{}}}
\bibcite{1}{{2}{n.d.}{{\em  \text  {Classification for Credit Card Default, by Theos Evgeniou and Spyros Zoumpoulis}}}{{}}}
\bibcite{breastcancer}{{3}{2015}{{Fombellida et~al.}}{{Fombellida, Torres-Alegre, Pi{\~{n}}uela-Izquierdo and\ Andina}}}
\bibcite{4}{{4}{n.d.}{{\em  \text  {FYS-STK4155 Project 1 at UiO, by G. S. Cabrera and B. S. Dalen}}}{{}}}
\bibcite{5}{{5}{n.d.}{{\em  \text  {FYS-STK4155 Project 1 at UiO, by S. H. Magn\IeC {\'u}sson and S. H\IeC {\r a}pnes}}}{{}}}
\bibcite{He2015}{{6}{2015}{{He et~al.}}{{He, Zhang, Ren and\ Sun}}}
\bibcite{ReLUImagenet}{{7}{2012}{{Krizhevsky et~al.}}{{Krizhevsky, Sutskever and\ Hinton}}}
\bibcite{NNWsunshine}{{8}{n.d.}{{Mellit et~al.}}{{Mellit, Benghanem and\ Bendekhis}}}
\bibcite{lecturenotes}{{9}{2015}{{Morten}}{{}}}
\bibcite{nielsenneural}{{10}{2018}{{Nielsen}}{{}}}
\bibcite{NNWfacedetection}{{11}{1998}{{Rowley et~al.}}{{Rowley, Baluja and\ Kanade}}}
\bibcite{stackexchangeminimum}{{12}{n.d.}{{user39663}}{{}}}
\bibcite{CCdata}{{13}{2009}{{Yeh and\ hui Lien}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {}References}{15}{section*.50}}
\@writefile{toc}{\contentsline {section}{\numberline {}References}{15}{section*.51}}
\newlabel{LastBibItem}{{13}{15}{}{section*.51}{}}
\bibstyle{agsm}
\newlabel{LastPage}{{}{15}{}{page.15}{}}
\xdef\lastpage@lastpage{15}
\xdef\lastpage@lastpageHy{15}
\@writefile{toc}{\contentsline {section}{\numberline {}Appendix A:\\OLS Predictor Derivation}{16}{section*.52}}
\newlabel{eq:appA_beta-rel}{{76}{16}{}{equation.6.76}{}}
