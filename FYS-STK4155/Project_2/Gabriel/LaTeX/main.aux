\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{NNWfacedetection}
\citation{NNWsunshine}
\citation{CCdata}
\citation{4}
\citation{5}
\newlabel{FirstPage}{{}{1}{}{section*.1}{}}
\@writefile{toc}{\contentsline {title}{Neural Networks}{1}{section*.2}}
\@writefile{toc}{\contentsline {abstract}{Abstract}{1}{section*.1}}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section*.3}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Theory and Algorithms}{1}{section*.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Linear Regression}{1}{section*.5}}
\citation{5}
\citation{5}
\citation{nielsenneural}
\citation{breastcancer}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Ordinary Least Squares}{2}{section*.6}}
\newlabel{eq:MSE}{{3}{2}{}{equation.2.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Ridge Regression}{2}{section*.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Logistic Regression and Gradient Descent}{2}{section*.8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Stochastic Gradient Descent method}{2}{section*.9}}
\citation{stackexchangeminimum}
\citation{lecturenotes}
\newlabel{eq:sigmoid}{{9}{3}{}{equation.2.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Local minimum vs. Global minimum of non-convex cost function conceptualization figure.}}{3}{figure.1}}
\newlabel{fig:local_min}{{1}{3}{Local minimum vs. Global minimum of non-convex cost function conceptualization figure}{figure.1}{}}
\citation{lecturenotes}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Neural Network Dynamics}{4}{section*.10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Feed-Forward}{4}{section*.11}}
\newlabel{eq:z1}{{18}{4}{}{equation.2.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces An illustration of how the inputs $X$ and weights $W$ and bias $b$ feed into a single neuron. Image taken from Antti Honkela.}}{4}{figure.2}}
\newlabel{fig:MLP_conc}{{2}{4}{An illustration of how the inputs $X$ and weights $W$ and bias $b$ feed into a single neuron. Image taken from Antti Honkela}{figure.2}{}}
\newlabel{eq:a1}{{19}{4}{}{equation.2.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A fully connected ANN illustration of two hidden layers and one output neuron. Image taken from: Vojtech Pavlovsky}}{4}{figure.3}}
\newlabel{fig:MLP_full}{{3}{4}{A fully connected ANN illustration of two hidden layers and one output neuron. Image taken from: Vojtech Pavlovsky}{figure.3}{}}
\newlabel{eq:zWb}{{23}{5}{}{equation.2.23}{}}
\newlabel{eq:afz}{{24}{5}{}{equation.2.24}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Backwards-Propagation}{5}{section*.12}}
\newlabel{eq:NNWdC/dW}{{28}{5}{}{equation.2.28}{}}
\newlabel{eq:NNWdC/db}{{29}{5}{}{equation.2.29}{}}
\newlabel{eq:bp_pdvs}{{30}{5}{}{equation.2.30}{}}
\newlabel{eq:C/zl}{{34}{5}{}{equation.2.34}{}}
\newlabel{eq:C/zl1}{{36}{5}{}{equation.2.36}{}}
\newlabel{eq:C/zl2}{{37}{5}{}{equation.2.37}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3}Learning rate, batches and $l2$ regularization}{6}{section*.13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D}Cost- and Activation functions}{7}{section*.14}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Several cost function examples. Mainly, the cross-entropy and mean-squared error (MSE) functions will be explored in this study. As explained previously, the activations of the final layer $a^L$ are equivalent with the prediction of $y$, namely $\mathaccentV {hat}05E{y}$. }}{7}{table.1}}
\newlabel{tab:cost_functions}{{I}{7}{Several cost function examples. Mainly, the cross-entropy and mean-squared error (MSE) functions will be explored in this study. As explained previously, the activations of the final layer $a^L$ are equivalent with the prediction of $y$, namely $\hat {y}$}{table.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Mean-Squared Error}{7}{section*.15}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Cross-Entropy}{7}{section*.16}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Table listing a number of common activation functions and their derivatives. Several of these are of interest and are implemented into the study.}}{7}{table.2}}
\newlabel{tab:activation_functions}{{II}{7}{Table listing a number of common activation functions and their derivatives. Several of these are of interest and are implemented into the study}{table.2}{}}
\citation{ReLUImagenet}
\citation{He2015}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3}Sigmoid function}{8}{section*.17}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4}Tanh function}{8}{section*.18}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5}ReLU6}{8}{section*.19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6}Weight- and Bias Initialization}{8}{section*.20}}
\newlabel{eq:xavier}{{55}{8}{}{equation.2.55}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {E}Classification}{8}{section*.21}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Neural Network Application}{8}{section*.22}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Accuracy Assertion of Classification}{8}{section*.23}}
\citation{6}
\citation{2}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Conceptualized figure of a cumulative gain chart. The figure includes perfect $100\%$ accuracy model, and a baseline illustrating what a random $50\%$ guess looks like. Image taken from Tom\IeC {\'a}\IeC {\v s} Jurczyk.}}{9}{figure.4}}
\newlabel{fig:conceptual_cumulative_gains}{{4}{9}{Conceptualized figure of a cumulative gain chart. The figure includes perfect $100\%$ accuracy model, and a baseline illustrating what a random $50\%$ guess looks like. Image taken from Tom치코 Jurczyk}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Conceptualized figure of the ROC curve. This figure also includes perfect $100\%$ accuracy model, and a baseline illustrating what a random $50\%$ guess looks like. Image taken from Tom\IeC {\'a}\IeC {\v s} Jurczyk}}{9}{figure.5}}
\newlabel{fig:conceptual_ROC}{{5}{9}{Conceptualized figure of the ROC curve. This figure also includes perfect $100\%$ accuracy model, and a baseline illustrating what a random $50\%$ guess looks like. Image taken from Tom치코 Jurczyk}{figure.5}{}}
\newlabel{eq:sensitivity}{{58}{9}{}{equation.2.58}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {F}Regression}{9}{section*.24}}
\newlabel{eq:frankes_function}{{63}{9}{}{equation.2.63}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The \textit  {Franke function} surface plot illustration for $x$ and $y$ values ranging from zero to one.}}{10}{figure.6}}
\newlabel{fig:frankesfunction}{{6}{10}{The \textit {Franke function} surface plot illustration for $x$ and $y$ values ranging from zero to one}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces The \textit  {Franke function} surface plot illustration for $x$ and $y$ values ranging from zero to one. Random Gaussian noise with mean value $\mu =0$ and standard deviation $\sigma =0.01$ is added to each data point $f(x_i,y_i)$.}}{10}{figure.7}}
\newlabel{fig:frankesfunction}{{7}{10}{The \textit {Franke function} surface plot illustration for $x$ and $y$ values ranging from zero to one. Random Gaussian noise with mean value $\mu =0$ and standard deviation $\sigma =0.01$ is added to each data point $f(x_i,y_i)$}{figure.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Neural Network Application}{10}{section*.25}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Accuracy Assertion of Regression}{10}{section*.26}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Method}{10}{section*.27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Preprocessing of the Data Set}{10}{section*.28}}
\citation{CCdata}
\citation{1}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Outlier Filtration}{11}{section*.29}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Column Scaling}{11}{section*.30}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3}One-Hot Encoding classifiers}{11}{section*.31}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4}History of past payment columns}{11}{section*.32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Neural Network Design}{12}{section*.33}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Overview of Neural Network Class}{12}{section*.34}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Logistic Regression/Stochastic Gradient Descent}{12}{section*.35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Hyperparameter and Learning Rate analysis}{12}{section*.36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D}Classification}{12}{section*.37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {E}Regression}{13}{section*.38}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Results}{13}{section*.39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Classification}{13}{section*.40}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The accuracy scores of the SGD algorithm for different $\eta $ and $\lambda $ values. The maximum accuracy score of the array is $0.612$, using $\lambda =10^{-6}$}}{13}{figure.8}}
\newlabel{fig:SGDacc}{{8}{13}{The accuracy scores of the SGD algorithm for different $\eta $ and $\lambda $ values. The maximum accuracy score of the array is $0.612$, using $\lambda =10^{-6}$}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The F1 scores of the SGD algorithm for different $\eta $ and $\lambda $ values. The maximum F1 score of the array is $0.510$, using $\lambda =3.594\cdot 10^{-7}$}}{13}{figure.9}}
\newlabel{fig:SGDf1}{{9}{13}{The F1 scores of the SGD algorithm for different $\eta $ and $\lambda $ values. The maximum F1 score of the array is $0.510$, using $\lambda =3.594\cdot 10^{-7}$}{figure.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The AUC scores of the SGD algorithm for different $\eta $ and $\lambda $ values. The maximum accuracy score of the array is $0.726$, using $\lambda =10^{-8}$}}{14}{figure.10}}
\newlabel{fig:SGDauc}{{10}{14}{The AUC scores of the SGD algorithm for different $\eta $ and $\lambda $ values. The maximum accuracy score of the array is $0.726$, using $\lambda =10^{-8}$}{figure.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces The gains ratios of the SGD algorithm for different $\eta $ and $\lambda $ values. The maximum accuracy score of the array is $0.466$, using $\lambda =4.642\cdot 10^{-8}$}}{14}{figure.11}}
\newlabel{fig:SGDgs}{{11}{14}{The gains ratios of the SGD algorithm for different $\eta $ and $\lambda $ values. The maximum accuracy score of the array is $0.466$, using $\lambda =4.642\cdot 10^{-8}$}{figure.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces The accuracy score of the classification neural network for different $\eta $ and $\lambda $ values. The maximum accuracy score of the array is $0.719$, using $\eta =0.056$ and $\lambda =10^{-4}$}}{14}{figure.12}}
\newlabel{fig:cc_acc}{{12}{14}{The accuracy score of the classification neural network for different $\eta $ and $\lambda $ values. The maximum accuracy score of the array is $0.719$, using $\eta =0.056$ and $\lambda =10^{-4}$}{figure.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces The F1 score of the classification neural network for different $\eta $ and $\lambda $ values. The maximum F1 score of the array is $0.484$, using $\eta =0.103$ and $\lambda =10^{-7}$}}{14}{figure.13}}
\newlabel{fig:cc_F1}{{13}{14}{The F1 score of the classification neural network for different $\eta $ and $\lambda $ values. The maximum F1 score of the array is $0.484$, using $\eta =0.103$ and $\lambda =10^{-7}$}{figure.13}{}}
\citation{4}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces The AUC-score of the classification neural network for different $\eta $ and $\lambda $ values. The maximum AUC-score of the array is $0.686$, using $\eta =0.150$ and $\lambda =2.154\cdot 10^{-7}$}}{15}{figure.14}}
\newlabel{fig:cc_auc}{{14}{15}{The AUC-score of the classification neural network for different $\eta $ and $\lambda $ values. The maximum AUC-score of the array is $0.686$, using $\eta =0.150$ and $\lambda =2.154\cdot 10^{-7}$}{figure.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces The gains-ratio of the classification neural network for different $\eta $ and $\lambda $ values. The maximum gains-ratio of the array is $0.465$, using $\eta \in [0.010, 0.103]$ and $\lambda =10^{-7}$}}{15}{figure.15}}
\newlabel{fig:cc_gr}{{15}{15}{The gains-ratio of the classification neural network for different $\eta $ and $\lambda $ values. The maximum gains-ratio of the array is $0.465$, using $\eta \in [0.010, 0.103]$ and $\lambda =10^{-7}$}{figure.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Regression}{15}{section*.41}}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces Table listing the final results and comparisons of the regressional methods applied to Franke's function.}}{15}{table.3}}
\newlabel{tab:conclusion_table_Frankes}{{III}{15}{Table listing the final results and comparisons of the regressional methods applied to Franke's function}{table.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces The MSE scores of the neural network applied to Franke's function for different $\eta $ and $\lambda $ values. The minimum MSE value of the array is 0.20, using $\eta \in [0.08, 0.2]$ and $\lambda =10^{-6}$.}}{15}{figure.16}}
\newlabel{fig:ff_mse}{{16}{15}{The MSE scores of the neural network applied to Franke's function for different $\eta $ and $\lambda $ values. The minimum MSE value of the array is 0.20, using $\eta \in [0.08, 0.2]$ and $\lambda =10^{-6}$}{figure.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces The R2 scores of the neural network applied to Franke's function for different $\eta $ and $\lambda $ values. The maximum MSE value of the array is 0.37, using $\eta \in [0.08, 0.2]$ and $\lambda =10^{-6}$.}}{15}{figure.17}}
\newlabel{fig:ff_r2}{{17}{15}{The R2 scores of the neural network applied to Franke's function for different $\eta $ and $\lambda $ values. The maximum MSE value of the array is 0.37, using $\eta \in [0.08, 0.2]$ and $\lambda =10^{-6}$}{figure.17}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Discussion}{15}{section*.42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Classification}{15}{section*.43}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Regression}{16}{section*.44}}
\bibdata{mainNotes,ref}
\bibcite{2}{{1}{n.d.}{{\em  \text  {Classification Accuracy is Not Enough: More Performance Measures You Can Use By Jason Brownlee}}}{{}}}
\bibcite{1}{{2}{n.d.}{{\em  \text  {Classification for Credit Card Default, by Theos Evgeniou and Spyros Zoumpoulis}}}{{}}}
\bibcite{breastcancer}{{3}{2015}{{Fombellida et~al.}}{{Fombellida, Torres-Alegre, Pi{\~{n}}uela-Izquierdo and\ Andina}}}
\bibcite{4}{{4}{n.d.}{{\em  \text  {FYS-STK4155 Project 1 at UiO, by G. S. Cabrera and B. S. Dalen}}}{{}}}
\bibcite{5}{{5}{n.d.}{{\em  \text  {FYS-STK4155 Project 1 at UiO, by S. H. Magn\IeC {\'u}sson and S. H\IeC {\r a}pnes}}}{{}}}
\bibcite{6}{{6}{n.d.}{{\em  \text  {'Gains vs ROC curves. Do you understand the difference?' by Tom\IeC {\'a}\IeC {\v s} Jurczyk}}}{{}}}
\bibcite{He2015}{{7}{2015}{{He et~al.}}{{He, Zhang, Ren and\ Sun}}}
\bibcite{ReLUImagenet}{{8}{2012}{{Krizhevsky et~al.}}{{Krizhevsky, Sutskever and\ Hinton}}}
\bibcite{NNWsunshine}{{9}{n.d.}{{Mellit et~al.}}{{Mellit, Benghanem and\ Bendekhis}}}
\bibcite{lecturenotes}{{10}{2015}{{Morten}}{{}}}
\bibcite{nielsenneural}{{11}{2018}{{Nielsen}}{{}}}
\bibcite{NNWfacedetection}{{12}{1998}{{Rowley et~al.}}{{Rowley, Baluja and\ Kanade}}}
\bibcite{stackexchangeminimum}{{13}{n.d.}{{user39663}}{{}}}
\bibcite{CCdata}{{14}{2009}{{Yeh and\ hui Lien}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusion}{17}{section*.45}}
\@writefile{toc}{\contentsline {section}{\numberline {}References}{17}{section*.46}}
\@writefile{toc}{\contentsline {section}{\numberline {}References}{17}{section*.47}}
\newlabel{LastBibItem}{{14}{17}{}{section*.47}{}}
\bibstyle{agsm}
\newlabel{LastPage}{{}{17}{}{page.17}{}}
\xdef\lastpage@lastpage{17}
\xdef\lastpage@lastpageHy{17}
\@writefile{toc}{\contentsline {section}{\numberline {}Appendix A:\\OLS Predictor Derivation}{18}{section*.48}}
\newlabel{eq:appA_beta-rel}{{76}{18}{}{equation.6.76}{}}
