\begin{abstract}
    A research project on the efficiency and performance of neural networks is presented. Network systems are compared to both stochastic gradient descent for a logistic case, as well as the Ridge regression scheme for a linear regression case. Taiwanese banking data is used for logistic regression, while Franke's function is used for the linear regression. A central focus of the project is which hyperparameters $\lambda$ and $\eta$ (\textit{l2} regularization and \textit{learning rate}) produce the best predictions for the neural network. It was found that the neural network outperformed the stochastic gradient descent algorithm, reproducing the testing data with a $71.9\%$ accuracy using $\lambda = 10^{-4}$ and $\eta = 0.056$. The ridge regression scheme was also found to be superior to the neural network design. The Ridge regression produced a minimum of $2\times 10^{-4}$, while the neural network produced a minimum of $0.2$ at best.
\end{abstract}
