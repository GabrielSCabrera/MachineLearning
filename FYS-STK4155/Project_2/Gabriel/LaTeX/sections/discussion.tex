\section{Discussion}
    \subsection{Classification}
        Figure \ref{fig:SGDacc} illustrates the binary accuracy grid search of the SGD algorithm. The best accuracy obtained with this algorithm is found to be $61.2\%$, using the hyperparameter $\lambda = 10^{-6}$. Interestingly enough, the learning rate $\eta$ does not seem to have an impact on the accuracy of the prediction. This may point to the fact that the first element in the learning rate array used in the grid search is sufficient for a minimum. Decreasing the learning rate beyond this is not jeopardizing to reaching the minimum, as long as you give the algorithm a large enough amount of iterations (what was previously referred to as the 'maxiter') to get there.\\\\
        Figure \ref{fig:SGDf1} illustrates the F1 scores of the SGD algorithm applied to the credit card data. The learning rate $\eta$ is once again quite irrelevant for the scores, as was found in the accuracy scores. The values do not differ significantly depending the regularization parameter $\lambda$, as was found in the accuracy measurements, though they do not increase or decrease systematically for the F1 scores. The scores seem to go up and down between $[0.509, 0.510]$, where a maximum is found at F1=0.510 using the regularization parameter $\lambda=3.594\cdot10^{-7}$.\\\\
        Figures \ref{fig:SGDauc} and \ref{fig:SGDgs} illustrate the AUC scores and gains ratios for the SGD algorithm. Once again we find that the learning rate $\eta$ does not have a noticeable impact on these parameters. Both these charts do not seems to have trends for increasing or decreasing regularization parameters $\lambda$. However, the gains ratio scores seem to have a convex maximum point shape, where the maximum is found to be $0.466$ for the hyperparameter $\lambda=4.642\cdot 10^{-8}$. Most of the scores are the same on this graph due to the choice of significant figures. It would be interesting to perform an analysis of these values using a larger decimal point precision.\\\\
        Figure \ref{fig:cc_acc} illustrates the success of the neural network algorithm developed for the study. The accuracy scores peak at an accuracy of nearly $72\%$ using the hyperparameter $\lambda = 10^{-4}$ and learning rate $\eta = 0.06$. The impact of both the hyperparameter $\lambda$ and learning rate $\eta$ is also well illustrated in this figure, as the values vary quite a bit in both the x- and y-axes. There is once again a general trend of decreasing accuracy for decreasing $\lambda$ values, just as in the SGD accuracy scores illustrated in figure \ref{fig:SGDacc}. It is more difficult to see the trends of the learning rate axis, though this is due to it not spanning as large a space as the $\lambda$ axis. It would be interesting to continue this study by increasing the span of the $\eta$ axis, perhaps revealing some larger pattern in how it impacts the prediction accuracy.\\\\ 
        Figure \ref{fig:cc_F1} illustrates the F1 scores for the ANN credit card data grid search. These values are underwhelming as the F1 score is expected to be close to 1 for good model results. These F1 scores are also worse than the SGD algorithm, pointing to something likely being implemented incorrectly in the analysis. Further work would be needed for this analysis. A slight trend of decreasing F1 scores is seen for decreasing $\lambda$ values, the trend going in the opposite direction of the accuracy scores. This is quite unusual as both the accuracy and F1 scores should measure the accuracy of the prediction. It may be that this is due to the definition of the F1 score. It is defined in terms of the true positives, but does not account for the true negatives at all. As we saw with the credit card data, nearly $80\%$ of the $n$ cases were zeros (not default). This was solved for the training set by upsampling the data, but the testing set is kept to this ratio. The reason for not upsampling the testing data is because the accuracy would be disingenuous. If a 'correctly' predicted data set is upsampled, then we would count that single correct prediction as multiple ones, causing the resulting accuracy estimation to be overly optimistic. The testing ratio is therefore kept to $80\%$ zeros, resulting in the strange F1 score behavior.\\\\
        Figures \ref{fig:cc_auc} and \ref{fig:cc_gr} illustrate the AUC scores and the gains ratios for the ANN applied to the credit card data. Both these graphs share a trend of increasing scores with decreasing hyperparameter values $\lambda$. Despite the $\lambda$ parameter being the dominant one in the grids, there is a clear interplay between $\lambda$ and $\eta$ to be seen. The AUC-Score chart illustrates this quite well, as a trade-off between the two parameters is found at $\eta=$ and $\lambda=$, where the AUC has a maximum of $0.686$. This same trade-off is however not as expressed in the gains ratio data, where the learning rate does not impact the scores much. This is best seen by the gains ratio maximum found, where 6 grid points produced the maximum result $0.465$. This result was obtained using $\lambda=10^{-7}$, and any learning rate in the list of $\eta \in \{ 0.010, 0.026, 0.041, 0.057, 0.072, 0.088, 0.103 \}$. Once again, the small impact of the learning rate may be due to its small span, or it may also be due to the model chosen. The learning rate has set to be constant throughout the report, though there are several other methods such as the ... and ... mentioned in the theory section. This would be interesting to study further.
            
    \subsection{Regression}
    	The results of the previous study are listed in table \ref{tab:conclusion_table_Frankes}. The lasso regression results were removed from the table as the lasso scheme is not included in the current comparison. The Ridge regression scheme accomplished a MSE minimum of $MSE_{min}= 2 \times 10^{-4}$ using a polynomial degree $p_{deg}=9$ and hyperparameter $\log\lambda=10^{-16}$. \\\\
		In \ref{fig:ff_mse}, we see that an $MSE = 0.2$ is optimal for low $\eta$ and $\lambda$; compared to the results of implementing polynomial regression in Project 1, these results are lackluster (values as low as 0.005 were found in Project 1,) and imply that the methods implemented in Project 1 are significantly faster and more effective at matching the Franke function.  The results shown in \ref{fig:ff_r2} follow the same trend, with the optimal $R^2$ at the same point as the optimal $MSE$. \\\\
		It is possible that better results could be obtained â€“ the layer configuration and number of epochs are two more parameters that can affect the efficacy of an ANN, so perhaps reducing the value of $\eta$ further while increasing the number of epochs would lead to smaller values of the $MSE$, and larger values for the $R^2$-score.  Additionally, increasing the model complexity by adding more layers, and varying the size of each layer might also improve performance.  We can, however, be certain that polynomial regression will not require the amount of time, nor amount of processing power that a neural network does.
			
      
        % COMPARE the two methods: 
            % Give a critical discussion of the results obtained with LR and NN. 
            % Make an analysis of the regularization parameters and learning rates employed to find the optimal accuracy score.
            % Compare choice of cost-functions for the two: did we choose different ones?
     
    %\subsection{Comparison of the Methods}
    	
        % Summarize the various algorithms and include a critical evaluation of their pros and cons. Which algorithm is best for the regression and which is best for the classification.

        % May have been a good idea to add custom weight and bias initialization for each layer added? Would have solved the input mess.
        % There are several factors of the study which could have been performed better; some of which include data preprocessing, grid search rather than ..., and stochasticity between epochs.
        % F1 score is 'ill defined' if the network declares all values to be zeros
        % Should have maybe implemented MPI4PY (code parallelization), and
        % Should have maybe performed some cross validation to find the optimal parameters. This would however have been very computationally inefficient.