\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{NNWfacedetection}
\citation{NNWsunshine}
\citation{CCdata}
\newlabel{FirstPage}{{}{1}{}{section*.1}{}}
\@writefile{toc}{\contentsline {title}{Neural Networks}{1}{section*.2}}
\@writefile{toc}{\contentsline {abstract}{Abstract}{1}{section*.1}}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section*.3}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Theory and Algorithms}{1}{section*.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Linear Regression}{1}{section*.5}}
\citation{nielsenneural}
\citation{breastcancer}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Ordinary Least Squares}{2}{section*.6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Ridge Regression}{2}{section*.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Logistic Regression and Gradient Descent}{2}{section*.8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Stochastic Gradient Descent method}{2}{section*.9}}
\citation{stackexchangeminimum}
\citation{lecturenotes}
\newlabel{eq:sigmoid}{{9}{3}{}{equation.2.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Local minimum vs. Global minimum of non-convex cost function conceptualization figure.}}{3}{figure.1}}
\newlabel{fig:local_min}{{1}{3}{Local minimum vs. Global minimum of non-convex cost function conceptualization figure}{figure.1}{}}
\citation{lecturenotes}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Neural Network Dynamics}{4}{section*.10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Feed-Forward}{4}{section*.11}}
\newlabel{eq:z1}{{18}{4}{}{equation.2.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces An illustration of how the inputs $X$ and weights $W$ and bias $b$ feed into a single neuron. Image taken from Antti Honkela.}}{4}{figure.2}}
\newlabel{fig:MLP_conc}{{2}{4}{An illustration of how the inputs $X$ and weights $W$ and bias $b$ feed into a single neuron. Image taken from Antti Honkela}{figure.2}{}}
\newlabel{eq:a1}{{19}{4}{}{equation.2.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A fully connected ANN illustration of two hidden layers and one output neuron. Image taken from: Vojtech Pavlovsky}}{4}{figure.3}}
\newlabel{fig:MLP_full}{{3}{4}{A fully connected ANN illustration of two hidden layers and one output neuron. Image taken from: Vojtech Pavlovsky}{figure.3}{}}
\newlabel{eq:zWb}{{23}{5}{}{equation.2.23}{}}
\newlabel{eq:afz}{{24}{5}{}{equation.2.24}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Backwards-Propagation}{5}{section*.12}}
\newlabel{eq:NNWdC/dW}{{28}{5}{}{equation.2.28}{}}
\newlabel{eq:NNWdC/db}{{29}{5}{}{equation.2.29}{}}
\newlabel{eq:bp_pdvs}{{30}{5}{}{equation.2.30}{}}
\newlabel{eq:C/zl}{{34}{5}{}{equation.2.34}{}}
\newlabel{eq:C/zl1}{{36}{5}{}{equation.2.36}{}}
\newlabel{eq:C/zl2}{{37}{5}{}{equation.2.37}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3}Learning rate, batches and $l2$ regularization}{6}{section*.13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D}Cost- and Activation functions}{7}{section*.14}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Several cost function examples. Mainly, the cross-entropy and mean-squared error (MSE) functions will be explored in this study. As explained previously, the activations of the final layer $a^L$ are equivalent with the prediction of $y$, namely $\mathaccentV {hat}05E{y}$. }}{7}{table.1}}
\newlabel{tab:cost_functions}{{I}{7}{Several cost function examples. Mainly, the cross-entropy and mean-squared error (MSE) functions will be explored in this study. As explained previously, the activations of the final layer $a^L$ are equivalent with the prediction of $y$, namely $\hat {y}$}{table.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Mean-Squared Error}{7}{section*.15}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Cross-Entropy}{7}{section*.16}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Table listing a number of common activation functions and their derivatives. Several of these are of interest and are implemented into the study.}}{7}{table.2}}
\newlabel{tab:activation_functions}{{II}{7}{Table listing a number of common activation functions and their derivatives. Several of these are of interest and are implemented into the study}{table.2}{}}
\citation{ReLUImagenet}
\citation{He2015}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3}Sigmoid function}{8}{section*.17}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4}Tanh function}{8}{section*.18}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5}ReLU6}{8}{section*.19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6}Weight- and Bias Initialization}{8}{section*.20}}
\newlabel{eq:xavier}{{55}{8}{}{equation.2.55}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {E}Classification}{8}{section*.21}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Neural Network Application}{8}{section*.22}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Accuracy Assertion}{8}{section*.23}}
\citation{2}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Conceptualized figure of a cumulative gain chart. The figure includes all cases of 0 and 1 predictions, and a perfect 1 model. The model also includes a baseline illustrating what a random $50/50$ pick of 0's and 1's looks like.}}{9}{figure.4}}
\newlabel{fig:conceptual_cumulative_gains}{{4}{9}{Conceptualized figure of a cumulative gain chart. The figure includes all cases of 0 and 1 predictions, and a perfect 1 model. The model also includes a baseline illustrating what a random $50/50$ pick of 0's and 1's looks like}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {F}Regression}{9}{section*.24}}
\newlabel{eq:frankes_function}{{64}{9}{}{equation.2.64}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Neural Network Application}{9}{section*.25}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Method}{9}{section*.26}}
\citation{CCdata}
\citation{1}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Preprocessing of the Data Set}{10}{section*.27}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Outlier Filtration}{10}{section*.28}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Column Scaling}{10}{section*.29}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3}One-Hot Encoding classifiers}{10}{section*.30}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4}History of past payment columns}{10}{section*.31}}
\citation{lecturenotes}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5}Principle Component Analysis}{11}{section*.32}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Correlation matrix heatmap before PCA was performed. }}{11}{figure.5}}
\newlabel{fig:pre_PCA}{{5}{11}{Correlation matrix heatmap before PCA was performed}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Correlation matrix heatmap after PCA was performed. }}{11}{figure.6}}
\newlabel{fig:post_PCA}{{6}{11}{Correlation matrix heatmap after PCA was performed}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Neural Network Design}{12}{section*.33}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Weight- and Bias Initialization}{12}{section*.34}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Hyperparameter and Learning Rate analysis}{12}{section*.35}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Classification}{12}{section*.36}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Logistic Regression}{12}{section*.37}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Neural Network: Classification}{12}{section*.38}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D}Regression}{13}{section*.39}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Linear Regression}{13}{section*.40}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Neural Network: Regression}{13}{section*.41}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Results}{13}{section*.42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Classification}{13}{section*.43}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Regression}{13}{section*.44}}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces Table listing the final results and comparisons of the regressional methods applied to Franke's function with $n=10,000$ data points.}}{13}{table.3}}
\newlabel{tab:conclusion_table_Frankes}{{III}{13}{Table listing the final results and comparisons of the regressional methods applied to Franke's function with $n=10,000$ data points}{table.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Grid search with label $N3$ results. This figure illustrates the accuracy scores for several neural networks with various hyperparameters $\lambda $ and learning rates $\eta $. The range and resolutions of the parameters are in the title, while the indices of the axes range from largest to smallest.}}{13}{figure.7}}
\newlabel{fig:ANNREG1}{{7}{13}{Grid search with label $N3$ results. This figure illustrates the accuracy scores for several neural networks with various hyperparameters $\lambda $ and learning rates $\eta $. The range and resolutions of the parameters are in the title, while the indices of the axes range from largest to smallest}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Grid search with label $N3$ results. This figure illustrates the area under the cumulative gain curve for several neural networks with various hyperparameters $\lambda $ and learning rates $\eta $. The range and resolutions of the parameters are in the title, while the indices of the axes range from largest to smallest.}}{13}{figure.8}}
\newlabel{fig:ANNREG2}{{8}{13}{Grid search with label $N3$ results. This figure illustrates the area under the cumulative gain curve for several neural networks with various hyperparameters $\lambda $ and learning rates $\eta $. The range and resolutions of the parameters are in the title, while the indices of the axes range from largest to smallest}{figure.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Discussion}{13}{section*.45}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Classification}{13}{section*.46}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Regression}{13}{section*.47}}
\bibdata{mainNotes,ref.bib}
\bibcite{2}{{1}{n.d.}{{\em  \text  {Classification Accuracy is Not Enough: More Performance Measures You Can Use By Jason Brownlee}}}{{}}}
\bibcite{1}{{2}{n.d.}{{\em  \text  {Classification for Credit Card Default, by Theos Evgeniou and Spyros Zoumpoulis}}}{{}}}
\bibcite{breastcancer}{{3}{2015}{{Fombellida et~al.}}{{Fombellida, Torres-Alegre, Pi{\~{n}}uela-Izquierdo and\ Andina}}}
\bibcite{He2015}{{4}{2015}{{He et~al.}}{{He, Zhang, Ren and\ Sun}}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Grid search with label $N3$ results. This figure illustrates the $F1$ for several neural networks with various hyperparameters $\lambda $ and learning rates $\eta $. The range and resolutions of the parameters are in the title, while the indices of the axes range from largest to smallest.}}{14}{figure.9}}
\newlabel{fig:ANNREG3}{{9}{14}{Grid search with label $N3$ results. This figure illustrates the $F1$ for several neural networks with various hyperparameters $\lambda $ and learning rates $\eta $. The range and resolutions of the parameters are in the title, while the indices of the axes range from largest to smallest}{figure.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The ANN regression on Franke's function. With $MSE=4.08$.}}{14}{figure.10}}
\newlabel{fig:ANNREG}{{10}{14}{The ANN regression on Franke's function. With $MSE=4.08$}{figure.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Comparison of the Methods}{14}{section*.48}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusion}{14}{section*.49}}
\@writefile{toc}{\contentsline {section}{\numberline {}References}{14}{section*.50}}
\@writefile{toc}{\contentsline {section}{\numberline {}References}{14}{section*.51}}
\bibcite{ReLUImagenet}{{5}{2012}{{Krizhevsky et~al.}}{{Krizhevsky, Sutskever and\ Hinton}}}
\bibcite{NNWsunshine}{{6}{n.d.}{{Mellit et~al.}}{{Mellit, Benghanem and\ Bendekhis}}}
\bibcite{lecturenotes}{{7}{2015}{{Morten}}{{}}}
\bibcite{nielsenneural}{{8}{2018}{{Nielsen}}{{}}}
\bibcite{NNWfacedetection}{{9}{1998}{{Rowley et~al.}}{{Rowley, Baluja and\ Kanade}}}
\bibcite{stackexchangeminimum}{{10}{n.d.}{{user39663}}{{}}}
\bibcite{CCdata}{{11}{2009}{{Yeh and\ hui Lien}}{{}}}
\newlabel{LastBibItem}{{11}{15}{}{section*.51}{}}
\bibstyle{agsm}
\newlabel{LastPage}{{}{15}{}{page.15}{}}
\xdef\lastpage@lastpage{15}
\xdef\lastpage@lastpageHy{15}
\@writefile{toc}{\contentsline {section}{\numberline {}Appendix A:\\OLS Predictor Derivation}{16}{section*.52}}
\newlabel{eq:appA_beta-rel}{{73}{16}{}{equation.6.73}{}}
