% REMEMBER TO SET LANGUAGE!
\documentclass[a4paper,10pt,english]{article}
\usepackage[utf8]{inputenc}
\usepackage[margin=0.5in]{geometry}
\usepackage{cite}
\usepackage{braket}
\usepackage{enumitem}
\usepackage{upgreek}
\usepackage{multicol}
\usepackage{mhchem}

% Standard stuff
\usepackage{amsmath,amsthm, amssymb,graphicx,varioref,verbatim,amsfonts,geometry,esint,url,color}
% colors in text
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
% Hyper refs
\usepackage[colorlinks]{hyperref}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{circuitikz}

\usepackage{tikz}
\usepackage[framemethod=tikz]{mdframed}
\usepackage{tikz-3dplot}
\usetikzlibrary{matrix,calc}

\usepackage{bm}

\usepackage[export]{adjustbox}

\usepackage{subfig}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage[makeroom]{cancel}

\usepackage{tcolorbox}
\tcbuselibrary{most}

%%%% PREVENT EXTRA WHITESPACE IN SECTION TITLES
\usepackage{sectsty}
\sectionfont{\raggedright}
%%%%

%%%%%FOR THE enumitem PACKAGE
\setlist[enumerate]{label*=\arabic*.}
%%%%%

%%%%EXAMPLE ENVIRONMENT

\newtcolorbox
[auto counter,number within=section]{pabox}[2][]{
%
enhanced,colback=black!5!white, colframe=black, fuzzy shadow={0mm}{-4pt}{-0.5pt}{0.4mm}{black!60!white},
title=Example 
\thetcbcounter
: #2,#1}

\newcommand{\example}[2]{
\begin{pabox}[label={myautocounter}]{#1}
#2
\end{pabox}
}
%%%%%%%%%%%%%%%%%%%%%%

%%%% BOX EQUATION ENVIRONMENT

\newenvironment{boxequation}{
\begin{tcolorbox}[ams equation, enhanced, colback=black!50!green!10!white, colframe=black, fuzzy shadow={0mm}{-4pt}{-0.5pt}{0.4mm}{black!60!white}]}
{\end{tcolorbox}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%% BOX QUOTE ENVIRONMENT

\newenvironment{boxquote}{
\begin{tcolorbox}[enhanced, colback=black!50!green!10!white, colframe=black, fuzzy shadow={0mm}{-4pt}{-0.5pt}{0.4mm}{black!60!white}]
\begin{center}}
{\end{center}\end{tcolorbox}}

%%%%%%%%%%%%%%%%%%%%%%%%%%

% Document formatting
\setlength{\parindent}{0mm}
\setlength{\parskip}{1.5mm}


%Color scheme for listings
\usepackage{textcomp}
\definecolor{listinggray}{gray}{0.9}
\definecolor{lbcolor}{rgb}{0.9,0.9,0.9}

%Custom Math Operators
\DeclareMathOperator{\col}{col}
\newcommand{\colx}{\col x}

\DeclareMathOperator{\row}{row}
\newcommand{\rowx}{\row x}

\DeclareMathOperator{\nul}{nul}
\newcommand{\nulx}{\nul x}

\DeclareMathOperator{\rank}{rank}
\newcommand{\rankx}{\rank x}

\DeclareMathOperator{\Span}{span}
\newcommand{\Spanx}{\span x}

\DeclareMathOperator{\range}{range}
\newcommand{\rangex}{\range x}

\DeclareMathOperator{\dist}{dist}
\newcommand{\distx}{\dist x}

\DeclareMathOperator{\proj}{proj}
\newcommand{\projx}{\proj x}

\usepackage{listings}
\lstset{
	backgroundcolor=\color{lbcolor},
	tabsize=4,
	rulecolor=,
	language=python,
        basicstyle=\scriptsize,
        upquote=true,
        aboveskip={1.5\baselineskip},
        columns=fixed,
	numbers=left,
        showstringspaces=false,
        extendedchars=true,
        breaklines=true,
        prebreak = \raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
        frame=single,
        showtabs=false,
        showspaces=false,
        showstringspaces=false,
        identifierstyle=\ttfamily,
        keywordstyle=\color[rgb]{0,0,1},
        commentstyle=\color[rgb]{0.133,0.545,0.133},
        stringstyle=\color[rgb]{0.627,0.126,0.941}
        }
        
       
\newcounter{subproject}
\renewcommand{\thesubproject}{\alph{subproject}}
\newenvironment{subproj}{
\begin{description}
\item[\refstepcounter{subproject}(\thesubproject)]
}{\end{description}}


\definecolor{ubuntu_terminal_background}{RGB}{48,10,36}

\lstdefinestyle{DOS}
{
    backgroundcolor=\color{ubuntu_terminal_background},
    basicstyle=\scriptsize\color{white}\ttfamily
}

\lstnewenvironment{terminal}[1]
  {%
   \mdframed[backgroundcolor = ubuntu_terminal_background, innertopmargin = -0.4cm, innerbottommargin = -0.1cm, hidealllines = true, innerleftmargin = 0.2cm, innerrightmargin = 0.2cm]%
   \lstset{
     backgroundcolor=\color{ubuntu_terminal_background}, keywords={},
     basicstyle=\scriptsize\color{white}\ttfamily, frame = none, numbers=none
   }%
  }
  {\endmdframed}


%%%%%%%%%%%%%%  VECTORS TO BOLD
%\let\oldhat\hat
%\renewcommand{\vec}[1]{\mathbf{#1}}
%%%%%%%%%%%%%%  

%%%%%%%%%%%%%% UNIT VECTOR 
\newcommand{\uveci}{{\bm{\hat{\textnormal{\bfseries\i}}}}}
\newcommand{\uvecj}{{\bm{\hat{\textnormal{\bfseries\j}}}}}
\DeclareRobustCommand{\uvec}[1]{{%
  \ifcsname uvec#1\endcsname
     \csname uvec#1\endcsname
   \else
    \bm{\hat{\mathbf{#1}}}%
   \fi
}}
%%%%%%%%%%%%%
  
\usepackage{titlesec}

\titleformat{\chapter}[display]
  {\normalfont\bfseries\Large\raggedleft\color{black}}
  {\tikz[remember picture,overlay] \node[opacity=0.9,inner sep=0pt,anchor=north] at (current page.north){\includegraphics[width=\paperwidth,height=10cm]{header.png}};
    \MakeUppercase{\chaptertitlename}%
        \rlap{ \resizebox{!}{1.5cm}{\thechapter}}
  }
  {10pt}{\Huge}
\titlespacing*{\chapter}{0pt}{-30pt}{-5pt}

%Griffiths' style calligraphy font: use like $\scripty{r}$
\usepackage{calligra}
\DeclareMathAlphabet{\mathcalligra}{T1}{calligra}{m}{n}
\DeclareFontShape{T1}{calligra}{m}{n}{<->s*[2.2]callig15}{}
\newcommand{\scripty}[1]{\ensuremath{\mathcalligra{#1}}}

\begin{document}

\ctikzset{bipoles/length=.6cm}
\newcommand\esymbol[1]{\begin{circuitikz}
\draw (0,0) to [#1] (1,0); \end{circuitikz}}

\title{STK-IN4300 Mandatory Assignment 1}
\author{Gabriel Sigurd Cabrera}

\maketitle

\begin{multicols*}{2}

\section*{Problem 1}

In this exercise, we will be performing a comparison of regression methods on a dataset provided by the \textit{European Bioinformatics Institute} at \url{https://www.ebi.ac.uk/arrayexpress/experiments/E-GEOD-12288/}.  We will be exploring \textit{\textsc{lasso}} and ridge methods for a selection of hyperparameter ($\lambda$) values; with the help of visual aides, we will compare and contrast the dependency of the \textit{mean squared error} (or $MSE$) on $\lambda$, in order to gauge the models' relative efficacies.

The dataset we will be working with is comprised of an input array $\mathbf{X} \in \mathbb{R}^{222 \times 22283}$ with corresponding outputs $\mathbf{y} \in \mathbb{R}^{222}$; to analyze this data, a \texttt{python} script\footnote{Available in the \textbf{Appendix}.} was used in conjunction with the \texttt{rpy2}, \texttt{NumPy}, \texttt{multiprocessing}, and \texttt{sklearn} modules.  The data is normalized as follows:

\begin{equation*}
\mathbf{x}_\text{norm} = \frac{\mathbf{x} - \text{mean}(\mathbf{x})}{\text{std}(\mathbf{x})}
\end{equation*}

Using \texttt{sklearn}, implementing the \textsc{lasso} and ridge algorithms is simple; in Figure \ref{fig_1} we can see that each method's $MSE$ behaves differently as a function of the hyperparameter:

\begin{figure}[H]
	\centering  
	\includegraphics[width = 0.5\textwidth, center]{../a_1000_iter.pdf}
	\caption{The $MSE$ as a function of $\lambda$ for a 1-D polynomial regression using the \textsc{lasso} and ridge algorithms; averaged over 10-fold cross validation. With maximum 1000 \textsc{lasso} iterations}
	\label{fig_1}
\end{figure}

We see that the $MSE$ for \textsc{lasso} is often superior to the ridge's $MSE$, but not for all values.  Additionally, ridge regression becomes permanently superior for hyperparameter values greater than $\lambda \approx 300$.  Things change somewhat once the iteration maximum is restricted to smaller values:

\begin{figure}[H]
	\centering  
	\includegraphics[width = 0.5\textwidth, center]{../a_10_iter.pdf}
	\caption{The $MSE$ as a function of $\lambda$ for a 1-D polynomial regression using the \textsc{lasso} and ridge algorithms; averaged over 10-fold cross validation. With ten \textsc{lasso} iterations.}
	\label{fig_2}
\end{figure}

In Figure \ref{fig_2}, we see that \textsc{lasso} consistently becomes the better choice until $\lambda \approx 300$ – once the hyperparameter gets large enough, ridge regression is once again optimal.

\begin{table}[H]
\center
\begin{tabular}{l | l l}
Iteration Max. & Min. \textsc{lasso} $MSE$ & Optimal $\lambda$ \\
\hline
$10$ & $0.996$ & $0.147$ \\
$100$ & $0.988$ & $0.155$ \\
$1000$ & $0.987$ & $0.155$ \\
\end{tabular}
\caption{Comparing the \textsc{lasso} algorithm's benchmark values for varying iteration maxima.  The minimum $MSE$ for \textit{ridge} regression is $0.965$ at $\lambda = 977.192$.\label{table_1}}
\end{table}

In conclusion, it appears that ridge is the regression method that overall leads to the best performance, with an important caveat: for $\lambda < 300$, \textsc{lasso} might be superior, depending of the iteration maximum.  In addition, perhaps further increasing the interation maximum would decrease the minimum \textsc{lasso} $MSE$, as we see a trend implying this in Table \ref{table_1}; of course, the performance impact would be absolutely non-negligeable, as \textsc{lasso} is already computationally heavier than ridge for the cases tested in this report.

The program used to calculate and plot the $MSE$ is available online at \url{https://github.com/GabrielSCabrera/MachineLearning/blob/master/STK-IN4300/Oblig1/a.py}

\section*{Problem 2}

We are given the \textit{linearized} expression for the \textit{object function}:

\begin{equation}
\label{eq_0}
A \equiv \sum_{i=1}^{N} g^\prime(\mathbf{w}_\text{old}^\textsc{t} \mathbf{x}_i)^2 \left( \frac{y_i - g(\mathbf{w}_\text{old}^\textsc{t} \mathbf{x}_i)}{g^\prime(\mathbf{w}_\text{old}^\textsc{t} \mathbf{x}_i)} + \mathbf{w}_\text{old}^\textsc{t} \mathbf{x}_i - \mathbf{w}^\textsc{t} \mathbf{x}_i \right)^2
\end{equation}

Where $y_i, g, g^\prime \in \mathbb{R}$; we also have that $\mathbf{w}, \mathbf{w}_\text{old}, \mathbf{x}_i \in \mathbb{R}^{p \times 1}$ for $i = 1,2,...,N$.  In a practical sense, $N$ might represent the number of points in a dataset, with $p$ representing the number of features present in said dataset.

We are interested in minimizing the scalar-valued $A$; to accomplish this, we must take the derivative of $A$ with respect to $\mathbf{w}$. We will then set this derivative to zero, and solve for the smallest possible $\mathbf{w}$; we will call this value $\mathbf{w}_\text{min}$.

To accomplish this, we must redefine (\ref{eq_0}) such that its \textit{summation notation} is replaced with a \textit{vector/matrix} expression; we begin by redefining some terms.  

\begin{table}[H]
\center
\begin{tabular}{c}
$\mathbf{x}^\textsc{t} \equiv \begin{bmatrix} \mathbf{x}_1 & \mathbf{x}_2 & \cdots & \mathbf{x}_\textsc{n} \end{bmatrix}$ \\ \\
$p_i \equiv \frac{y_i - g(\mathbf{w}_\text{old}^\textsc{t} \mathbf{x}_i)}{g^\prime(\mathbf{w}_\text{old}^\textsc{t} \mathbf{x}_i)} + \mathbf{w}_\text{old}^\textsc{t} \mathbf{x}_i$ \\ \\
$\mathbf{p} \equiv \begin{bmatrix} p_1 & p_2 & \cdots & p_\textsc{n} \end{bmatrix}$ \\ \\
$q_i \equiv g^\prime(\mathbf{w}_\text{old}^\textsc{t} \mathbf{x}_i)^2$ \\ \\
$\mathbf{q} \equiv \begin{bmatrix} q_1 & q_2 & \cdots & q_\textsc{n} \end{bmatrix}$ \\ \\
$\mathbf{r} \equiv \text{diag}(\mathbf{q})^2$
\end{tabular}
\end{table}

This gives us\footnote{This can be verified to be true programmatically – see the \textbf{Appendix}.}

\begin{equation}
\label{eq_1}
A = \sum_{i=1}^{N} q_i ( p_i - \mathbf{w}^\textsc{t} \mathbf{x}_i )^2 = (\mathbf{p} - \mathbf{w}^\textsc{t} \mathbf{x}^\textsc{t}) \mathbf{r} (\mathbf{p} - \mathbf{w}^\textsc{t} \mathbf{x}^\textsc{t})^\textsc{t}
\end{equation}

We then expand the above, giving us several easily-differentiable terms:

\begin{equation*}
A = \mathbf{p} \mathbf{r} \mathbf{p}^\textsc{t} - \mathbf{p} \mathbf{r} \mathbf{x} \mathbf{w} - \mathbf{w}^\textsc{t} \mathbf{x}^\textsc{t} \mathbf{r} \mathbf{p}^\textsc{t} + \mathbf{w}^\textsc{t} \mathbf{x}^\textsc{t} \mathbf{r} \mathbf{x} \mathbf{w}
\end{equation*}

Since each term is a scalar, it is valid to replace each of them with their own transpose if need be. Consider the fact that $\mathbf{w}^\textsc{t} \mathbf{x}^\textsc{t} \mathbf{r}^\textsc{t} \mathbf{p}^\textsc{t} = (\mathbf{p} \mathbf{r} \mathbf{x} \mathbf{w})^\textsc{t}$; since $\mathbf{r}$ is a \textit{diagonal} matrix, we have that $\mathbf{r} = \mathbf{r}^\textsc{t}$, and so we can combine some terms:

\begin{equation*}
A = \mathbf{p} \mathbf{r} \mathbf{p}^\textsc{t} - 2 \mathbf{p} \mathbf{r} \mathbf{x} \mathbf{w} + \mathbf{w}^\textsc{t} \mathbf{x}^\textsc{t} \mathbf{r} \mathbf{x} \mathbf{w}
\end{equation*}

Next, we differentiate with respect to $\mathbf{w}$, keeping in mind the rule $\nabla_\mathbf{x} \mathbf{x}^\textsc{t} \mathbf{v} \mathbf{x} = 2 \mathbf{v} \mathbf{x}$ for $\mathbf{v}$ independent of $\mathbf{x}$:

\begin{equation*}
\nabla_\mathbf{w} A = - 2 \mathbf{p} \mathbf{r} \mathbf{x} + 2 \mathbf{x}^\textsc{t} \mathbf{r} \mathbf{x} \mathbf{w}
\end{equation*}

Setting the above equal to zero allows us to minimize $\mathbf{w}$:

\begin{equation*}
0 = - 2 \mathbf{p} \mathbf{r} \mathbf{x} + 2 \mathbf{x}^\textsc{t} \mathbf{r} \mathbf{x} \mathbf{w}_\text{min}
\end{equation*}

Finally in (\ref{eq_2}), we are left with our desired result:

\begin{equation}
\label{eq_2}
\mathbf{w}_\text{min} = ( \mathbf{x}^\textsc{t} \mathbf{r} \mathbf{x} )^{-1} ( \mathbf{p} \mathbf{r} \mathbf{x} )
\end{equation}

\section*{Appendix}

We can use \texttt{python} to verify that (\ref{eq_1}) holds; below is a script that will generate ten-thousand sets of randomly shaped arrays containing random values – these sets consist of the matrix $\mathbf{x}$, and vectors $\mathbf{w}$, $\mathbf{p}$, and $\mathbf{q}$.  Using the \texttt{NumPy} and \texttt{multiprocessing} modules, we calculate the difference between the values calculated by both sides of (\ref{eq_1}) for each set of initial conditions.

\begin{lstlisting}[showstringspaces=false,language=Python,numbers=none]
from multiprocessing import Pool
import numpy as np

np.random.seed(123)

def test(dummy):
    N = np.random.randint(5, 50)    # Number of datapoints
    p = np.random.randint(2, 6)     # Number of features
    X = np.random.random((N,p))     # Matrix x
    W = np.random.random((p,1))     # Vector w
    P = np.random.random((1,N))     # Vector p
    Q = np.random.random((1,N))     # Vector q

    # Evaluating the summation form
    summation_total = 0
    for i in range(N):
        x_i = X[i,:,np.newaxis]     # Vector x_i
        summation_total += (Q[:,i]**2)*((P[:,i] - W.T @ x_i)**2)

    root_R = np.zeros((N,N))
    for i in range(N):
        root_R[i,i] = Q[:,i]
    R = root_R @ root_R             # Matrix r

    # Evaluating the vector form
    vector_total = (P - W.T @ X.T) @ R @ (P - W.T @ X.T).T

    # Calculating and saving the difference between each total
    difference = np.squeeze(np.abs(summation_total - vector_total))
    return difference

N_tests = 1E4

# Running tests a total of "N_tests" times
pool = Pool()
results = np.array(pool.map(test, (None for i in range(int(N_tests)))))

# Gathering information on the results and printing
max_res, mean_res, median_res = \
np.max(results), np.mean(results), np.median(results)
print("Information on differences between summation total and vector total")
print(f"\tMaximum: {max_res}\n\tMean: {mean_res}\n\tMedian: {median_res}")
\end{lstlisting}

The output is as follows:

\begin{terminal}

Information on differences between summation total and vector total
	Maximum: 1.0658141036401503e-14
	Mean: 5.56243939797696e-16
	Median: 1.1102230246251565e-16
\end{terminal}

Clearly, the only differences are due to numerical error; our assertion that (\ref{eq_1}) holds is therefore near-certain to be true.

\end{multicols*}

\end{document}
