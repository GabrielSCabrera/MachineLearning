\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{scikit-learn}
\citation{pygam}
\citation{matplotlib}
\citation{scipy}
\citation{pandas}
\citation{numpy}
\newlabel{FirstPage}{{}{1}{}{Doc-Start}{}}
\@writefile{toc}{\contentsline {title}{STK-IN4300 Mandatory Assignment 2}{1}{section*.1}}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{section*.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Packages}{1}{section*.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Datasets}{1}{section*.4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Forced Vital Capacity}{1}{section*.5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Diabetes}{1}{section*.6}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Problem 1}{1}{section*.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Task 1}{1}{section*.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Task 2}{1}{section*.9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Task 3}{1}{section*.10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Task 4}{2}{section*.11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Task 5}{2}{section*.12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Task 6}{2}{section*.13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Task 7}{2}{section*.14}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Problem 2}{2}{section*.15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Task 1}{2}{section*.16}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Task 2}{2}{section*.17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Task 3}{2}{section*.18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}Task 4}{3}{section*.19}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Appendix}{3}{section*.20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Tables}{3}{section*.21}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces The features of the \textit  {diabetes} dataset.}}{3}{table.1}}
\newlabel{table_1}{{I}{3}{The features of the \textit {diabetes} dataset}{table.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces The features of the \textit  {forced vital capacity} dataset.}}{3}{table.2}}
\newlabel{table_0}{{II}{3}{The features of the \textit {forced vital capacity} dataset}{table.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces Information on each coefficient in the \textit  {vector of coefficients}, for a simple linear regression. $MSE = 9.8266 \times 10^{-3}$.}}{3}{table.3}}
\newlabel{table_2}{{III}{3}{Information on each coefficient in the \textit {vector of coefficients}, for a simple linear regression. $MSE = 9.8266 \times 10^{-3}$}{table.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {IV}{\ignorespaces Remaining features after backward elimination with stopping criterion $p \geq 0.7$. $MSE = 9.8286 \times 10^{-3}$}}{3}{table.4}}
\newlabel{table_3}{{IV}{3}{Remaining features after backward elimination with stopping criterion $p \geq 0.7$. $MSE = 9.8286 \times 10^{-3}$}{table.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {V}{\ignorespaces Remaining features after forward selection with stopping criterion $p \geq 0.7$. $MSE = 9.7452 \times 10^{-3}$}}{4}{table.5}}
\newlabel{table_4}{{V}{4}{Remaining features after forward selection with stopping criterion $p \geq 0.7$. $MSE = 9.7452 \times 10^{-3}$}{table.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {VI}{\ignorespaces Remaining features after backward elimination with stopping criterion $p \geq 0.8$. $MSE = 9.8754 \times 10^{-3}$}}{4}{table.6}}
\newlabel{table_5}{{VI}{4}{Remaining features after backward elimination with stopping criterion $p \geq 0.8$. $MSE = 9.8754 \times 10^{-3}$}{table.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {VII}{\ignorespaces Remaining features after forward selection with stopping criterion $p \geq 0.8$. $MSE = 1.0055 \times 10^{-2}$}}{4}{table.7}}
\newlabel{table_6}{{VII}{4}{Remaining features after forward selection with stopping criterion $p \geq 0.8$. $MSE = 1.0055 \times 10^{-2}$}{table.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {VIII}{\ignorespaces The vector of coefficients for a linear regression boosting model.}}{4}{table.8}}
\newlabel{table_7}{{VIII}{4}{The vector of coefficients for a linear regression boosting model}{table.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {IX}{\ignorespaces The MSE for each method applied in Problem 1, ordered from best to worst performance.}}{4}{table.9}}
\newlabel{table_8}{{IX}{4}{The MSE for each method applied in Problem 1, ordered from best to worst performance}{table.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {X}{\ignorespaces The cumulative accuracies for each feature in the diabetes dataset (ordered from highest p-value to lowest).}}{4}{table.10}}
\newlabel{table_9}{{X}{4}{The cumulative accuracies for each feature in the diabetes dataset (ordered from highest p-value to lowest)}{table.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {XI}{\ignorespaces The accuracy scores for a variety of classification methods, sorted from best to worst. }}{4}{table.11}}
\newlabel{table_10}{{XI}{4}{The accuracy scores for a variety of classification methods, sorted from best to worst}{table.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Figures}{5}{section*.22}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A performance comparison between k-nearest-neighbors cross-validation and bootstrap, as a function of the complexity parameter $\alpha $.}}{5}{figure.1}}
\newlabel{fig_1}{{1}{5}{A performance comparison between k-nearest-neighbors cross-validation and bootstrap, as a function of the complexity parameter $\alpha $}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A performance comparison between k-nearest-neighbors cross-validation and leave-one-out cross-validation, as a function of the number of neighbors $k$.}}{5}{figure.2}}
\newlabel{fig_2}{{2}{5}{A performance comparison between k-nearest-neighbors cross-validation and leave-one-out cross-validation, as a function of the number of neighbors $k$}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Source Code}{5}{section*.23}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1}Snippets}{5}{section*.24}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A preprocessing script for the \textit  {forced vital capacity} dataset.}}{5}{figure.3}}
\newlabel{code_0}{{3}{5}{A preprocessing script for the \textit {forced vital capacity} dataset}{figure.3}{}}
\bibdata{oblig2Notes,bib}
\bibcite{scikit-learn}{{1}{}{{}}{{}}}
\bibcite{pygam}{{2}{}{{}}{{}}}
\bibcite{matplotlib}{{3}{}{{}}{{}}}
\bibcite{scipy}{{4}{}{{}}{{}}}
\bibcite{pandas}{{5}{}{{}}{{}}}
\bibcite{numpy}{{6}{}{{}}{{}}}
\bibstyle{ieeetr}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\newlabel{LastPage}{{}{5}{}{page.5}{}}
\xdef\lastpage@lastpage{5}
\xdef\lastpage@lastpageHy{5}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2}Complete Source Code}{6}{section*.25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {}References}{6}{section*.26}}
\@writefile{toc}{\contentsline {section}{\numberline {}References}{6}{section*.27}}
\newlabel{LastBibItem}{{6}{6}{}{section*.27}{}}
